{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guideline\n",
    "\n",
    "![](example_guideline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference  \n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hidden layer까지 있는 경우  \n",
    "- adjacency matrix의 일종인 mat_mask를 통해서 hidden layer들의 dimension 정보를 담고 있는 hidden_dim_list를 산출  \n",
    "\n",
    "__1. Architecture을 짜는 코드는 다음과 같이 세 파트__   \n",
    "\n",
    "- input layer와 첫번째 hidden layer을 이어줌(connection별로)\n",
    "- hidden layer와 hidden layer들끼리 이어줌(반복문으로 가야함, 총 # of hidden layer - 1 만큼 반복)\n",
    "- 마지막 hidden layer와 output layer를 이어줌 \n",
    "\n",
    "__2. layer와 layer을 이어줄 때__   \n",
    "\n",
    "- 나중 layer의 노드들 기준으로 반복이 이루어져야함(forward시에는 value가 나중 layer의 노드로 합쳐지면서 흘러가니까)   \n",
    "- 각 connection에 대해서 activation function이 다르기 때문에 node 기준으로 architecture을 짜 주어야 함 -> node dictionary를 쓸건데, __backprop이 가능하게 하기 위해서는 어떤 자료형?을 써야 하는지 고민해야__  \n",
    "    - node = {}\n",
    "    - node\\['hidden1', ~~~\\]\n",
    "\n",
    "__3. 추가 고려사항__   \n",
    "\n",
    "- skip connection 사용가능할건지, 사용한다면 구현상으로 어떻게 달라질지   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 0, 0, 2, 0, 0, 0, 0, 0],\n",
      "        [2, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 2, 0, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 3, 0]], dtype=torch.int32)\n",
      "torch.Size([7, 10])\n"
     ]
    }
   ],
   "source": [
    "# mat_mask 행렬 바로 생성 \n",
    "mat_mask = np.array([[0,2,0,0,2,0,0,0,0,0],[2,0,2,0,0,0,0,0,0,0],[0,2,0,2,0,0,0,0,0,0],[0,0,0,0,0,1,1,0,0,0],[0,0,0,0,0,0,1,1,0,0],[0,0,0,0,0,0,0,0,0,3],[0,0,0,0,0,0,0,0,3,0]])\n",
    "mat_mask = torch.from_numpy(mat_mask)\n",
    "print(mat_mask)\n",
    "print(mat_mask.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer 추가한 버전\n",
    "#아래 코드를 좀 변형하면 될듯? 아래 코드는 hidden layer가 없어서.. \n",
    "class MAT_to_WANN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, mat_mask):\n",
    "        \"\"\"\n",
    "       :param in_features: number of input features\n",
    "       :param out_features: number of output features\n",
    "       :param indices_mask: list of two lists containing indices for dimensions 0 and 1, used to create the mask  \n",
    "       \"\"\"\n",
    "        super(MaskedLinear, self).__init__()    \n",
    "        \n",
    "        def calculate_hidden_dim(in_dim, out_dim, mat_mask):\n",
    "            #hidden dim의 정보를 담은 list를 출력한다..?\n",
    "            hidden_dim_list = []\n",
    "            start_col_idx = 0 \n",
    "            finish_col_idx = in_dim - 1\n",
    "            \n",
    "            while(True):\n",
    "                if finish_col_idx >= mat_mask.shape[1]:\n",
    "                    break \n",
    "        \n",
    "                for i in range(start_col_idx, len(mat_mask)): \n",
    "                    if (mat_mask[i,start_col_idx:(finish_col_idx + 1)].sum() == 0):\n",
    "                        hidden_dim = i - sum(hidden_dim_list)\n",
    "                        hidden_dim_list += [hidden_dim]  \n",
    "                        start_col_idx = finish_col_idx + 1\n",
    "                        finish_col_idx += i \n",
    "                        break \n",
    "                        \n",
    "            return hidden_dim_list\n",
    "        \n",
    "    \n",
    "    \n",
    "    #이부분 바꿔야함    \n",
    "    def forward(self, input):   \n",
    "        \n",
    "        #각 layer별로, 그 안에서 노드별로 전 layer에서 값들을 받아서 더해져야함\n",
    "        #아래는 아직 수도코드임 \n",
    "        #첫번째 hidden layer의 노드별로 선언  \n",
    "        hidden1_node1_value = nn.ReLU(nn.Linear(1,1)(input_node2_value)) + nn.ReLU(nn.Linear(1,1)(input_node4_value))  \n",
    "        hidden1_node2_value = nn.ReLU(nn.Linear(1,1)(input_node3_value))  \n",
    "        hidden1_node3_value = nn.ReLU(nn.Linear(1,1)(input_node1)) + nn.ReLU(nn.Linear(1,1)(input_node4))  \n",
    "        \n",
    "        #두번째 hidden layer의 노드별로 선언\n",
    "        hidden2_node1_value = nn.Linear(1,1)(hidden1_node1_value) + nn.Linear(1,1)(hidden1_node2_value) \n",
    "        hidden2_node2_value = nn.Linear(1,1)(hidden1_node2_value) + nn.Linear(1,1)(hidden1_note3_value)  \n",
    "        \n",
    "        #output layer의 노드별로 선언\n",
    "        output_node1_value = nn.Sigmoid(nn.Linear(1,1)(hidden2_node2_value))\n",
    "        output_node2_value = nn.Sigmoid(nn.Linear(1,1)(hidden2_node1_value)) \n",
    "        \n",
    "        out = torch.concat([output_node1_value, output_node2_value])  \n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
