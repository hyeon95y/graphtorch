{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert given array as a sparse connected fcn\n",
    "\n",
    "- Input node cannot be connected to the others\n",
    "- But all output nodes should be connected to at least one hidden node or input node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection also represents its acitvation function\n",
    "- 0 : not connected\n",
    "- 1 : linear \n",
    "- 2 : ReLU\n",
    "- 3 : Sigmoid\n",
    "- and so on..\n",
    "\n",
    "**this index can be changed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 : Not connected\n",
    "# 1 : linear\n",
    "# 2 : ReLU\n",
    "# 3 : Sigmoid\n",
    "activations = [None, None, nn.ReLU(), nn.Sigmoid()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Concatenate layer output with additional input data](https://discuss.pytorch.org/t/concatenate-layer-output-with-additional-input-data/20462)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixForWANN():\n",
    "    \n",
    "    def __init__(self, mat, in_dim, out_dim):\n",
    "        # get when initialized\n",
    "        self.mat = mat\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        #calculate\n",
    "        self.num_hidden_nodes = self.mat.shape[1] - self.in_dim   \n",
    "        \n",
    "        #when matrix has hidden layer  \n",
    "        if self.num_hidden_nodes == 1:  \n",
    "            self.hidden_dim = [1] \n",
    "        elif self.num_hidden_nodes == 0:\n",
    "            self.hidden_dim = []\n",
    "        else:\n",
    "            self.hidden_dim = self.get_hidden_dim()   \n",
    "            \n",
    "            \n",
    "    def get_hidden_dim(self):\n",
    "        in_dim = self.in_dim\n",
    "        out_dim = self.out_dim\n",
    "        mat_mask = self.mat\n",
    "        \n",
    "        hidden_dim_list = []\n",
    "        start_col_idx = 0\n",
    "        finish_col_idx = in_dim -1   \n",
    "        \n",
    "        while(True):\n",
    "            \n",
    "            if finish_col_idx >= mat_mask.shape[1]:   \n",
    "                print(finish_col_idx)\n",
    "                break  \n",
    "            \n",
    "            if ((mat_mask.shape[0] - sum(hidden_dim_list)) == out_dim):  #example4 해결\n",
    "                 break  #지금 hidden dimension들 합이랑 output dim 합이 row길이랑 같으면 더이상 탐색 필요 x\n",
    "            \n",
    "            for i in range(sum(hidden_dim_list), len(mat_mask)): #이부분이상한데..?   \n",
    "    \n",
    "                #밑에처럼 하면 example 2에서 오류가 남.\n",
    "                #skip connection에 대한 예외처리 해줘야 함   \n",
    "    \n",
    "                if(mat_mask[i,start_col_idx:(finish_col_idx + 1)].sum() == 0):   \n",
    "                \n",
    "                    hidden_dim = i - sum(hidden_dim_list)\n",
    "                    hidden_dim_list += [hidden_dim]\n",
    "                    start_col_idx = finish_col_idx + 1\n",
    "                    finish_col_idx += hidden_dim   \n",
    "                    break    \n",
    "                    \n",
    "        return hidden_dim_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/example1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/example1_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = np.array([[0,2,0,0,2,0,0,0,0,0],\n",
    "                [2,0,2,0,0,0,0,0,0,0],\n",
    "                [0,2,0,2,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,1,1,0,0,0],\n",
    "                [0,0,0,0,0,0,1,1,0,0],\n",
    "                [0,0,0,0,0,0,0,0,0,3],\n",
    "                [0,0,0,0,0,0,0,0,3,0]])\n",
    "in_dim = 5\n",
    "out_dim = 2\n",
    "mat_wann1 = MatrixForWANN(mat1, in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 2, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first position represents row : FROM\n",
    "mat1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second position represents column : TO\n",
    "mat1[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 0, 2],\n",
       "       [2, 0, 2, 0, 0],\n",
       "       [0, 2, 0, 2, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get destinations of input layer\n",
    "mat1[:mat_wann1.num_hidden_nodes, :in_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of total hidden nodes\n",
    "mat_wann1.num_hidden_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of hidden layers and its node index\n",
    "mat_wann1.hidden_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/example2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/example2_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-60567607edee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0min_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmat_wann2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatrixForWANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-edd73fafc546>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mat, in_dim, out_dim)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# when matrix has hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0min_dim\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hidden_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-edd73fafc546>\u001b[0m in \u001b[0;36mget_hidden_dim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_col_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmat_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_col_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinish_col_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mat2 = np.array([[2,0,0,0,0,0],\n",
    "                [0,0,0,3,0,1]])\n",
    "in_dim = 5\n",
    "out_dim = 1\n",
    "mat_wann2 = MatrixForWANN(mat2, in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mat_wann2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3ebe9ded3290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmat_wann2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mat_wann2' is not defined"
     ]
    }
   ],
   "source": [
    "mat_wann2.num_hidden_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mat_wann2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-58c1a8fd8fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmat_wann2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mat_wann2' is not defined"
     ]
    }
   ],
   "source": [
    "mat_wann2.hidden_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/example3_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat3 = np.array([[2,0,0,3,0],\n",
    "                [0,0,0,0,1]])\n",
    "in_dim = 5\n",
    "out_dim = 2\n",
    "mat_wann3 = MatrixForWANN(mat3, in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_wann3.num_hidden_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_wann3.hidden_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/example4_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat4 = np.array([[3,0,1,0,0],\n",
    "                [0,3,0,0,0],\n",
    "                [0,0,0,2,0],\n",
    "                [0,0,0,0,2],\n",
    "                [0,0,1,2,0],\n",
    "                [0,0,1,0,0]])\n",
    "in_dim = 3\n",
    "out_dim = 4\n",
    "mat_wann4 = MatrixForWANN(mat4, in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_wann4.num_hidden_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_wann4.hidden_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5\n",
    "- Example 1에서 Input4 -> Hidden 5로 가는 Skip connection이 추가됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/example5_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat5 = np.array([[0,2,0,0,2,0,0,0,0,0],\n",
    "                [2,0,2,0,0,0,0,0,0,0],\n",
    "                [0,2,0,2,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,1,1,0,0,0],\n",
    "                [0,0,0,2,0,0,1,1,0,0],\n",
    "                [0,0,0,0,0,0,0,0,0,3],\n",
    "                [0,0,0,0,0,0,0,0,3,0]])\n",
    "in_dim = 5\n",
    "out_dim = 2\n",
    "mat_wann5 = MatrixForWANN(mat5, in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class WANNFCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_activation(x, idx_activation, activations) :  \n",
    "    if idx_activation == 0 :\n",
    "        assert True\n",
    "    elif idx_activation == 1 :\n",
    "        return nn.Linear(1,1)(x)\n",
    "    else : \n",
    "        return activations[idx_activation](nn.Linear(1,1)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(784, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without hidden layer counts\n",
    "\n",
    "class WANNFCN(nn.Module) : \n",
    "    def __init__(self, mat_wann, activations) : \n",
    "        super(WANNFCN, self).__init__()\n",
    "        self.mat = mat_wann.mat\n",
    "        self.in_dim = mat_wann.in_dim\n",
    "        self.out_dim = mat_wann.out_dim\n",
    "        self.num_hidden_nodes = mat_wann.num_hidden_nodes\n",
    "        self.hidden_dim = mat_wann.hidden_dim\n",
    "        \n",
    "        self.activations = activations\n",
    "        \n",
    "        self.nodes = {}\n",
    "        '''\n",
    "        nodes라는 dictionary 안에 아래와 같이 저장됨\n",
    "        'hidden_1' : 해당 노드\n",
    "        'hidden_2' : 해당 노드\n",
    "        ...\n",
    "        'output_1' : 해당 output 노드, hidden node로부터 연결되어있음\n",
    "        'output_2' : 해당 output 노드, input node, hidden node로부터 연결되어있음\n",
    "        '''\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        \n",
    "        # hidden node가 한개라도 있을때\n",
    "        if self.num_hidden_nodes != 0 :\n",
    "            self.to_hidden(x)\n",
    "        \n",
    "        # output은 반드시 있음\n",
    "        outputs = self.to_output(x)\n",
    "        print(self.nodes)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def to_output(self, x) :\n",
    "        return ''\n",
    "    \n",
    "    def to_hidden(self, x) : \n",
    "        # input layer와 모든 이전 hidden layer를 탐색\n",
    "        # 그렇지 않으면 skip connection을 놓칠수 있음\n",
    "        # 모든 node와 connection은 dictionary self.nodes에 저장\n",
    "        print(self.hidden_dim)\n",
    "        hidden_node_counts = 0\n",
    "        \n",
    "        \n",
    "        ############################### loop for hidden nodes\n",
    "        for idx_hidden_row in list(range(0, self.mat.shape[0])) : \n",
    "            #connections_from_input = self.mat[idx_hidden_row, :self.in_dim]\n",
    "            connections_from_input = self.mat[idx_hidden_row, :]\n",
    "            print('connection from input : ', connections_from_input)\n",
    "            if connections_from_input.sum() != 0 :\n",
    "                count_connection = 0\n",
    "                input_node = None\n",
    "                ############################# loop for input nodes\n",
    "                for idx_input_col, activation_type in enumerate(connections_from_input) :\n",
    "                    print('idx_input_col %s, activation_type %s' % (idx_input_col, activation_type))\n",
    "                    if activation_type != 0 and count_connection == 0:\n",
    "                        # x[sample index, positional index for input]\n",
    "                        print('\\n**first input node')\n",
    "\n",
    "                        # 1) idx_input_col 이 input에서 오는 경우\n",
    "                        if idx_input_col < self.in_dim : \n",
    "                            input_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "                        # 2) idx_input_col이 hidden에서 오는 경우\n",
    "                        elif idx_input_col >= self.in_dim : \n",
    "                            input_node = wrap_activation(self.nodes['hidden_%d'%(idx_input_col-self.in_dim)], activation_type, activations)\n",
    "\n",
    "                        print(input_node)\n",
    "                        count_connection += 1\n",
    "                    elif activation_type != 0 and count_connection != 0 :\n",
    "                        print('%s input node' % idx_input_col)\n",
    "                        # x[sample index, positional index for input]\n",
    "                        # torch.sum returns the addition of two tensors\n",
    "\n",
    "                        print('\\n**input_node', input_node.shape)\n",
    "                        print(input_node)\n",
    "\n",
    "                        #new_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "\n",
    "                        new_node = None\n",
    "                        # 1) idx_input_col 이 input에서 오는 경우\n",
    "                        if idx_input_col < self.in_dim : \n",
    "                            new_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "                        # 2) idx_input_col이 hidden에서 오는 경우\n",
    "                        elif idx_input_col >= self.in_dim : \n",
    "                            new_node = wrap_activation(self.nodes['hidden_%d'%(idx_input_col-self.in_dim)], activation_type, activations)\n",
    "\n",
    "\n",
    "\n",
    "                        print('\\n**wrap_activation', new_node.shape)\n",
    "                        print(new_node)\n",
    "                        input_node = input_node + new_node\n",
    "                        print('\\n**sum', input_node.shape)\n",
    "                        print(input_node)\n",
    "\n",
    "\n",
    "                        #input_node = torch.sum(input_node, wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations))\n",
    "                        count_connection += 1\n",
    "            # connect all input nodes to given hidden node\n",
    "            if idx_hidden_row < self.num_hidden_nodes : \n",
    "                self.nodes['hidden_%d'%idx_hidden_row] = input_node \n",
    "            else : \n",
    "                self.nodes['output_%d'%(idx_hidden_row-self.num_hidden_nodes)] = input_node \n",
    "        # sum all numbers of hidden nodes from this layer\n",
    "        hidden_node_counts += 1\n",
    "\n",
    "            \n",
    "            ############ 2. 두번째 이후의 layer일 경우 : input을 x로 받거나, 앞의 hidden node로 받름\n",
    "            #if idx_hidden_layer == 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "connection from input :  [0 2 0 0 2 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[0.3565],\n",
      "        [3.3295],\n",
      "        [6.3025]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 2\n",
      "4 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.3565],\n",
      "        [3.3295],\n",
      "        [6.3025]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[0.3565],\n",
      "        [3.3295],\n",
      "        [6.3025]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [2 0 2 0 0 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[0.0000],\n",
      "        [2.0731],\n",
      "        [4.3955]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 2\n",
      "2 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.0000],\n",
      "        [2.0731],\n",
      "        [4.3955]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[ 2.2581],\n",
      "        [ 6.3706],\n",
      "        [10.4832]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[ 2.2581],\n",
      "        [ 8.4437],\n",
      "        [14.8787]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [0 2 0 2 0 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[2.3941],\n",
      "        [6.1335],\n",
      "        [9.8729]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 2\n",
      "3 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[2.3941],\n",
      "        [6.1335],\n",
      "        [9.8729]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[ 4.3267],\n",
      "        [ 8.7255],\n",
      "        [13.1243]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[ 6.7208],\n",
      "        [14.8590],\n",
      "        [22.9972]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [0 0 0 0 0 1 1 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 1\n",
      "\n",
      "**first input node\n",
      "tensor([[1.1136],\n",
      "        [3.2295],\n",
      "        [5.3454]], grad_fn=<AddmmBackward>)\n",
      "idx_input_col 6, activation_type 1\n",
      "6 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[1.1136],\n",
      "        [3.2295],\n",
      "        [5.3454]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[0.2419],\n",
      "        [1.0835],\n",
      "        [1.9591]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[1.3555],\n",
      "        [4.3130],\n",
      "        [7.3045]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [0 0 0 0 0 0 1 1 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 1\n",
      "\n",
      "**first input node\n",
      "tensor([[0.5130],\n",
      "        [2.7169],\n",
      "        [5.0096]], grad_fn=<AddmmBackward>)\n",
      "idx_input_col 7, activation_type 1\n",
      "7 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.5130],\n",
      "        [2.7169],\n",
      "        [5.0096]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[1.1029],\n",
      "        [2.0396],\n",
      "        [2.9764]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[1.6159],\n",
      "        [4.7565],\n",
      "        [7.9860]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [0 0 0 0 0 0 0 0 0 3]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 3\n",
      "\n",
      "**first input node\n",
      "tensor([[0.1470],\n",
      "        [0.0132],\n",
      "        [0.0010]], grad_fn=<SigmoidBackward>)\n",
      "connection from input :  [0 0 0 0 0 0 0 0 3 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 3\n",
      "\n",
      "**first input node\n",
      "tensor([[0.5951],\n",
      "        [0.9341],\n",
      "        [0.9929]], grad_fn=<SigmoidBackward>)\n",
      "idx_input_col 9, activation_type 0\n",
      "{'hidden_0': tensor([[0.3565],\n",
      "        [3.3295],\n",
      "        [6.3025]], grad_fn=<AddBackward0>), 'hidden_1': tensor([[ 2.2581],\n",
      "        [ 8.4437],\n",
      "        [14.8787]], grad_fn=<AddBackward0>), 'hidden_2': tensor([[ 6.7208],\n",
      "        [14.8590],\n",
      "        [22.9972]], grad_fn=<AddBackward0>), 'hidden_3': tensor([[1.3555],\n",
      "        [4.3130],\n",
      "        [7.3045]], grad_fn=<AddBackward0>), 'hidden_4': tensor([[1.6159],\n",
      "        [4.7565],\n",
      "        [7.9860]], grad_fn=<AddBackward0>), 'output_0': tensor([[0.1470],\n",
      "        [0.0132],\n",
      "        [0.0010]], grad_fn=<SigmoidBackward>), 'output_1': tensor([[0.5951],\n",
      "        [0.9341],\n",
      "        [0.9929]], grad_fn=<SigmoidBackward>)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WANNFCN(mat_wann1, activations)\n",
    "\n",
    "numpy_input = np.array([[1,2,3,4,5],\n",
    "                        [6,7,8,9,10],\n",
    "                        [11,12,13,14,15]])\n",
    "\n",
    "#numpy_input = np.array([[1,2,3,4,5]])\n",
    "numpy_input = torch.from_numpy(numpy_input).float()\n",
    "model(numpy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with hidden layer counts\n",
    "\n",
    "class WANNFCN(nn.Module) : \n",
    "    def __init__(self, mat_wann, activations) : \n",
    "        super(WANNFCN, self).__init__()\n",
    "        self.mat = mat_wann.mat\n",
    "        self.in_dim = mat_wann.in_dim\n",
    "        self.out_dim = mat_wann.out_dim\n",
    "        self.num_hidden_nodes = mat_wann.num_hidden_nodes\n",
    "        self.hidden_dim = mat_wann.hidden_dim\n",
    "        \n",
    "        self.activations = activations\n",
    "        \n",
    "        self.nodes = {}\n",
    "        '''\n",
    "        nodes라는 dictionary 안에 아래와 같이 저장됨\n",
    "        'hidden_1' : 해당 노드\n",
    "        'hidden_2' : 해당 노드\n",
    "        ...\n",
    "        'output_1' : 해당 output 노드, hidden node로부터 연결되어있음\n",
    "        'output_2' : 해당 output 노드, input node, hidden node로부터 연결되어있음\n",
    "        '''\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        \n",
    "        # hidden node가 한개라도 있을때\n",
    "        if self.num_hidden_nodes != 0 :\n",
    "            self.to_hidden(x)\n",
    "        \n",
    "        # output은 반드시 있음\n",
    "        outputs = self.to_output(x)\n",
    "        print(self.nodes)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def to_output(self, x) :\n",
    "        return ''\n",
    "    \n",
    "    def to_hidden(self, x) : \n",
    "        # input layer와 모든 이전 hidden layer를 탐색\n",
    "        # 그렇지 않으면 skip connection을 놓칠수 있음\n",
    "        # 모든 node와 connection은 dictionary self.nodes에 저장\n",
    "        print(self.hidden_dim)\n",
    "        hidden_node_counts = 0\n",
    "        \n",
    "        ########################## loop for hidden layers\n",
    "        # self.hidden_dim : [3, 2]\n",
    "        for idx_hidden_layer, num_hidden_nodes in enumerate(self.hidden_dim) : \n",
    "            print('idx_hidden_layer : %s, num_hidden_nodes : %s' % (idx_hidden_layer, num_hidden_nodes))\n",
    "            \n",
    "            ########### 1. 첫번째 레이어일 경우 : input을 x로 받음\n",
    "            indices_hidden_row = []\n",
    "            if idx_hidden_layer == 0 :\n",
    "                indices_hidden_row = list(range(0, num_hidden_nodes))\n",
    "            else :\n",
    "                indices_hidden_row = list(range(hidden_node_counts, hidden_node_counts+num_hidden_nodes))\n",
    "            print('indices_hidden_row : %s' % indices_hidden_row)\n",
    "\n",
    "            ############################### loop for hidden nodes\n",
    "            for idx_hidden_row in indices_hidden_row : \n",
    "                #connections_from_input = self.mat[idx_hidden_row, :self.in_dim]\n",
    "                connections_from_input = self.mat[idx_hidden_row, :]\n",
    "                print('connection from input : ', connections_from_input)\n",
    "                if connections_from_input.sum() != 0 :\n",
    "                    count_connection = 0\n",
    "                    input_node = None\n",
    "                    ############################# loop for input nodes\n",
    "                    for idx_input_col, activation_type in enumerate(connections_from_input) :\n",
    "                        print('idx_input_col %s, activation_type %s' % (idx_input_col, activation_type))\n",
    "                        if activation_type != 0 and count_connection == 0:\n",
    "                            # x[sample index, positional index for input]\n",
    "                            print('\\n**first input node')\n",
    "\n",
    "                            # 1) idx_input_col 이 input에서 오는 경우\n",
    "                            if idx_input_col < self.in_dim : \n",
    "                                input_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "                            # 2) idx_input_col이 hidden에서 오는 경우\n",
    "                            elif idx_input_col >= self.in_dim : \n",
    "                                input_node = wrap_activation(self.nodes['hidden_%d'%(idx_input_col-self.in_dim)], activation_type, activations)\n",
    "\n",
    "                            print(input_node)\n",
    "                            count_connection += 1\n",
    "                        elif activation_type != 0 and count_connection != 0 :\n",
    "                            print('%s input node' % idx_input_col)\n",
    "                            # x[sample index, positional index for input]\n",
    "                            # torch.sum returns the addition of two tensors\n",
    "\n",
    "                            print('\\n**input_node', input_node.shape)\n",
    "                            print(input_node)\n",
    "\n",
    "                            #new_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "\n",
    "                            new_node = None\n",
    "                            # 1) idx_input_col 이 input에서 오는 경우\n",
    "                            if idx_input_col < self.in_dim : \n",
    "                                new_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "                            # 2) idx_input_col이 hidden에서 오는 경우\n",
    "                            elif idx_input_col >= self.in_dim : \n",
    "                                new_node = wrap_activation(self.nodes['hidden_%d'%(idx_input_col-self.in_dim)], activation_type, activations)\n",
    "\n",
    "\n",
    "\n",
    "                            print('\\n**wrap_activation', new_node.shape)\n",
    "                            print(new_node)\n",
    "                            input_node = input_node + new_node\n",
    "                            print('\\n**sum', input_node.shape)\n",
    "                            print(input_node)\n",
    "\n",
    "\n",
    "                            #input_node = torch.sum(input_node, wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations))\n",
    "                            count_connection += 1\n",
    "                # connect all input nodes to given hidden node\n",
    "                self.nodes['hidden_%d'%idx_hidden_row] = input_node \n",
    "\n",
    "            # sum all numbers of hidden nodes from this layer\n",
    "            hidden_node_counts += len(indices_hidden_row)\n",
    "\n",
    "            \n",
    "            ############ 2. 두번째 이후의 layer일 경우 : input을 x로 받거나, 앞의 hidden node로 받름\n",
    "            #if idx_hidden_layer == 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "idx_hidden_layer : 0, num_hidden_nodes : 3\n",
      "indices_hidden_row : [0, 1, 2]\n",
      "connection from input :  [0 2 0 0 2 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[0.0000],\n",
      "        [0.9916],\n",
      "        [1.9985]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 2\n",
      "4 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.0000],\n",
      "        [0.9916],\n",
      "        [1.9985]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[0.0000],\n",
      "        [0.9916],\n",
      "        [1.9985]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [2 0 2 0 0 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 2\n",
      "2 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[0.4532],\n",
      "        [2.1139],\n",
      "        [3.7746]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[0.4532],\n",
      "        [2.1139],\n",
      "        [3.7746]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [0 2 0 2 0 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 2\n",
      "3 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[0.2087],\n",
      "        [1.5388],\n",
      "        [2.8689]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[0.2087],\n",
      "        [1.5388],\n",
      "        [2.8689]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "idx_hidden_layer : 1, num_hidden_nodes : 2\n",
      "indices_hidden_row : [3, 4]\n",
      "connection from input :  [0 0 0 0 0 1 1 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 1\n",
      "\n",
      "**first input node\n",
      "tensor([[0.2608],\n",
      "        [0.9403],\n",
      "        [1.6303]], grad_fn=<AddmmBackward>)\n",
      "idx_input_col 6, activation_type 1\n",
      "6 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.2608],\n",
      "        [0.9403],\n",
      "        [1.6303]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[1.1837],\n",
      "        [2.6266],\n",
      "        [4.0695]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[1.4445],\n",
      "        [3.5669],\n",
      "        [5.6998]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [0 0 0 0 0 0 1 1 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 1\n",
      "\n",
      "**first input node\n",
      "tensor([[-1.3343],\n",
      "        [-2.6011],\n",
      "        [-3.8678]], grad_fn=<AddmmBackward>)\n",
      "idx_input_col 7, activation_type 1\n",
      "7 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[-1.3343],\n",
      "        [-2.6011],\n",
      "        [-3.8678]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[-0.9991],\n",
      "        [-2.2751],\n",
      "        [-3.5510]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[-2.3334],\n",
      "        [-4.8761],\n",
      "        [-7.4188]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "{'hidden_0': tensor([[0.0000],\n",
      "        [0.9916],\n",
      "        [1.9985]], grad_fn=<AddBackward0>), 'hidden_1': tensor([[0.4532],\n",
      "        [2.1139],\n",
      "        [3.7746]], grad_fn=<AddBackward0>), 'hidden_2': tensor([[0.2087],\n",
      "        [1.5388],\n",
      "        [2.8689]], grad_fn=<AddBackward0>), 'hidden_3': tensor([[1.4445],\n",
      "        [3.5669],\n",
      "        [5.6998]], grad_fn=<AddBackward0>), 'hidden_4': tensor([[-2.3334],\n",
      "        [-4.8761],\n",
      "        [-7.4188]], grad_fn=<AddBackward0>)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WANNFCN(mat_wann1, activations)\n",
    "\n",
    "numpy_input = np.array([[1,2,3,4,5],\n",
    "                        [6,7,8,9,10],\n",
    "                        [11,12,13,14,15]])\n",
    "\n",
    "#numpy_input = np.array([[1,2,3,4,5]])\n",
    "numpy_input = torch.from_numpy(numpy_input).float()\n",
    "model(numpy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WANNFCN(nn.Module) : \n",
    "    def __init__(self, mat_wann, activations) : \n",
    "        super(WANNFCN, self).__init__()\n",
    "        self.mat = mat_wann.mat\n",
    "        self.in_dim = mat_wann.in_dim\n",
    "        self.out_dim = mat_wann.out_dim\n",
    "        self.num_hidden_nodes = mat_wann.num_hidden_nodes\n",
    "        self.hidden_dim = mat_wann.hidden_dim\n",
    "        \n",
    "        self.activations = activations\n",
    "        \n",
    "        self.nodes = {}\n",
    "        '''\n",
    "        nodes라는 dictionary 안에 아래와 같이 저장됨\n",
    "        'hidden_1' : 해당 노드\n",
    "        'hidden_2' : 해당 노드\n",
    "        ...\n",
    "        'output_1' : 해당 output 노드, hidden node로부터 연결되어있음\n",
    "        'output_2' : 해당 output 노드, input node, hidden node로부터 연결되어있음\n",
    "        '''\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        \n",
    "        # hidden node가 한개라도 있을때\n",
    "        if self.num_hidden_nodes != 0 :\n",
    "            self.to_hidden(x)\n",
    "        \n",
    "        # output은 반드시 있음\n",
    "        outputs = self.to_output(x)\n",
    "        print(self.nodes)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def to_hidden(self, x) : \n",
    "        # input layer와 모든 이전 hidden layer를 탐색\n",
    "        # 그렇지 않으면 skip connection을 놓칠수 있음\n",
    "        # 모든 node와 connection은 dictionary self.nodes에 저장\n",
    "        print(self.hidden_dim)\n",
    "        hidden_node_counts = 0\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        ########################## loop for hidden layers\n",
    "        for idx_hidden_layer, num_hidden_nodes in enumerate(self.hidden_dim) : \n",
    "            print('idx_hidden_layer : %s, num_hidden_nodes : %s' % (idx_hidden_layer, num_hidden_nodes))\n",
    "            \n",
    "            \n",
    "            ############################# 1. First layer \n",
    "            # Check input layer only\n",
    "            #if idx_hidden_layer == 0 :\n",
    "            indices_hidden_row = []\n",
    "            if idx_hidden_layer == 0 :\n",
    "                indices_hidden_row = list(range(0, num_hidden_nodes))\n",
    "            else :\n",
    "                indices_hidden_row = list(range(hidden_node_counts, hidden_node_counts+num_hidden_nodes))\n",
    "            print('indices_hidden_row : %s' % indices_hidden_row)\n",
    "\n",
    "            ############################### loop for hidden nodes\n",
    "            for idx_hidden_row in indices_hidden_row : \n",
    "                #connections_from_input = self.mat[idx_hidden_row, :self.in_dim]\n",
    "                connections_from_input = self.mat[idx_hidden_row, :]\n",
    "                print('connection from input : ', connections_from_input)\n",
    "                if connections_from_input.sum() != 0 :\n",
    "                    count_connection = 0\n",
    "                    input_node = None\n",
    "                    ############################# loop for input nodes\n",
    "                    for idx_input_col, activation_type in enumerate(connections_from_input) :\n",
    "                        print('idx_input_col %s, activation_type %s' % (idx_input_col, activation_type))\n",
    "                        if activation_type != 0 and count_connection == 0:\n",
    "                            # x[sample index, positional index for input]\n",
    "                            print('\\n**first input node')\n",
    "                            input_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "                            print(input_node)\n",
    "                            count_connection += 1\n",
    "                        elif activation_type != 0 and count_connection != 0 :\n",
    "                            print('%s input node' % idx_input_col)\n",
    "                            # x[sample index, positional index for input]\n",
    "                            # torch.sum returns the addition of two tensors\n",
    "\n",
    "                            print('\\n**input_node', input_node.shape)\n",
    "                            print(input_node)\n",
    "\n",
    "                            new_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "                            print('\\n**wrap_activation', new_node.shape)\n",
    "                            print(new_node)\n",
    "                            input_node = input_node + new_node\n",
    "                            print('\\n**sum', input_node.shape)\n",
    "                            print(input_node)\n",
    "\n",
    "\n",
    "                            #input_node = torch.sum(input_node, wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations))\n",
    "                            count_connection += 1\n",
    "                # connect all input nodes to given hidden node\n",
    "                self.nodes['hidden_%d'%idx_hidden_row] = input_node \n",
    "\n",
    "            # sum all numbers of hidden nodes from this layer\n",
    "            hidden_node_counts += len(indices_hidden_row)\n",
    "                \n",
    "                \n",
    "            '''\n",
    "            ############################# 2. Other hidden layers\n",
    "            # Check input layer, previous hidden layers\n",
    "            else : \n",
    "                print('################ else')\n",
    "                # hidden node index (전체 히든 노드 중에서)\n",
    "                indices_hidden_row = list(range(hidden_node_counts, hidden_node_counts+num_hidden_nodes))\n",
    "                print('indices_hidden_row : %s' % indices_hidden_row)\n",
    "                \n",
    "                ############################### loop for hidden nodes\n",
    "                for idx_hidden_row in indices_hidden_row : \n",
    "                    #connections_from_input = self.mat[idx_hidden_row, :self.in_dim]\n",
    "                    connections_from_input = self.mat[idx_hidden_row, :]\n",
    "                    print('connection from input : ', connections_from_input)\n",
    "                    \n",
    "                    count_connection = 0\n",
    "                    input_node = None\n",
    "                    \n",
    "                    #################### input으로부터 오는 연결\n",
    "                    if connections_from_input.sum() != 0 :\n",
    "                        \n",
    "                        ############################# loop for input nodes\n",
    "                        for idx_input_col, activation_type in enumerate(connections_from_input) :\n",
    "                            print('idx_input_col %s, activation_type %s' % (idx_input_col, activation_type))\n",
    "                            if activation_type != 0 and count_connection == 0:\n",
    "                                # x[sample index, positional index for input]\n",
    "                                print('\\n**first input node')\n",
    "                                input_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "                                print(input_node)\n",
    "                                count_connection += 1\n",
    "                            elif activation_type != 0 and count_connection != 0 :\n",
    "                                print('%s input node' % idx_input_col)\n",
    "                                # x[sample index, positional index for input]\n",
    "                                # torch.sum returns the addition of two tensors\n",
    "                                \n",
    "                                print('\\n**input_node', input_node.shape)\n",
    "                                print(input_node)\n",
    "                                \n",
    "                                new_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "                                print('\\n**wrap_activation', new_node.shape)\n",
    "                                print(new_node)\n",
    "                                input_node = input_node + new_node\n",
    "                                print('\\n**sum', input_node.shape)\n",
    "                                print(input_node)\n",
    "                                \n",
    "                                \n",
    "                                #input_node = torch.sum(input_node, wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations))\n",
    "                                count_connection += 1  \n",
    "                        \n",
    "                    ###################### 앞의 hidden layer로부터 오는 연결\n",
    "                    #print('############# 앞의 hidden layer로부터 오는 연결 ###########')\n",
    "                    #print(self.hidden_dim, idx_hidden_layer)\n",
    "                    #for idx_hidden_row in indices_hidden_row : \n",
    "                    #    connections_from_input = self.mat[idx_hidden_row, :self.in_dim]\n",
    "                   \n",
    "                        \n",
    "                    # connect all input nodes to given hidden node\n",
    "                    self.nodes['hidden_%d'%idx_hidden_row] = input_node \n",
    "                \n",
    "                hidden_node_counts += len(indices_hidden_row)\n",
    "            '''\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def to_output(self, x) : \n",
    "        # input layer와 모든 이전 hidden layer를 탐색\n",
    "        # 그렇지 않으면 skip connection을 놓칠수 있음\n",
    "        # 모든 node와 connection은 dictionary self.nodes에 저장\n",
    "        \n",
    "        outputs = 'test'\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6683, 5.7021, 8.7358])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0., 0., 0.])\n",
    "torch.tensor([2.6683, 5.7021, 8.7358])\n",
    "torch.tensor([0., 0., 0.]) + torch.tensor([2.6683, 5.7021, 8.7358])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "torch.Size([3, 5])\n",
      "idx_hidden_layer : 0, num_hidden_nodes : 3\n",
      "indices_hidden_row : [0, 1, 2]\n",
      "connection from input :  [0 2 0 0 2 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[1.0502],\n",
      "        [1.3514],\n",
      "        [1.6527]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 2\n",
      "4 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[1.0502],\n",
      "        [1.3514],\n",
      "        [1.6527]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[ 3.4772],\n",
      "        [ 6.8423],\n",
      "        [10.2074]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[ 4.5274],\n",
      "        [ 8.1937],\n",
      "        [11.8601]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [2 0 2 0 0 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[0.0000],\n",
      "        [1.5141],\n",
      "        [3.4118]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 2\n",
      "2 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.0000],\n",
      "        [1.5141],\n",
      "        [3.4118]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[0.0000],\n",
      "        [1.5141],\n",
      "        [3.4118]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "connection from input :  [0 2 0 2 0 0 0 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 2\n",
      "\n",
      "**first input node\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 2\n",
      "3 input node\n",
      "\n",
      "**input_node torch.Size([3, 1])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**wrap_activation torch.Size([3, 1])\n",
      "tensor([[ 3.0655],\n",
      "        [ 6.8113],\n",
      "        [10.5571]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "**sum torch.Size([3, 1])\n",
      "tensor([[ 3.0655],\n",
      "        [ 6.8113],\n",
      "        [10.5571]], grad_fn=<AddBackward0>)\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 0\n",
      "idx_input_col 6, activation_type 0\n",
      "idx_input_col 7, activation_type 0\n",
      "idx_input_col 8, activation_type 0\n",
      "idx_input_col 9, activation_type 0\n",
      "idx_hidden_layer : 1, num_hidden_nodes : 2\n",
      "indices_hidden_row : [3, 4]\n",
      "connection from input :  [0 0 0 0 0 1 1 0 0 0]\n",
      "idx_input_col 0, activation_type 0\n",
      "idx_input_col 1, activation_type 0\n",
      "idx_input_col 2, activation_type 0\n",
      "idx_input_col 3, activation_type 0\n",
      "idx_input_col 4, activation_type 0\n",
      "idx_input_col 5, activation_type 1\n",
      "\n",
      "**first input node\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for dimension 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-728d64413378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#numpy_input = np.array([[1,2,3,4,5]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnumpy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-5a1eb0433557>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# hidden node가 한개라도 있을때\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_nodes\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# output은 반드시 있음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-5a1eb0433557>\u001b[0m in \u001b[0;36mto_hidden\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m                             \u001b[0;31m# x[sample index, positional index for input]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n**first input node'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                             \u001b[0minput_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_input_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                             \u001b[0mcount_connection\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for dimension 1 with size 5"
     ]
    }
   ],
   "source": [
    "model = WANNFCN(mat_wann1, activations)\n",
    "\n",
    "numpy_input = np.array([[1,2,3,4,5],\n",
    "                        [6,7,8,9,10],\n",
    "                        [11,12,13,14,15]])\n",
    "\n",
    "#numpy_input = np.array([[1,2,3,4,5]])\n",
    "numpy_input = torch.from_numpy(numpy_input).float()\n",
    "model(numpy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WANNFCN(mat_wann3, activations)\n",
    "numpy_input = np.array([[1,2,3,4,5],\n",
    "                        [6,7,8,9,10],\n",
    "                        [11,12,13,14,15]])\n",
    "numpy_input = torch.from_numpy(numpy_input).float()\n",
    "model(numpy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n",
      "idx_hidden_layer : 0, num_hidden_nodes : 2\n",
      "idx_hidden_row : [0, 1]\n",
      "[[3 0 1]\n",
      " [0 3 0]]\n",
      "idx_hidden_layer : 1, num_hidden_nodes : 3\n",
      "idx_hidden_row : [2, 3, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WANNFCN(mat_wann4, activations)\n",
    "numpy_input = np.array([[1,2,3,4],\n",
    "                        [6,7,8,9],\n",
    "                        [11,12,13,14]])\n",
    "numpy_input = torch.from_numpy(numpy_input).float()\n",
    "model(numpy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "idx_hidden_layer : 0, num_hidden_nodes : 3\n",
      "idx_hidden_row : [0, 1, 2]\n",
      "[[0 2 0 0 2]\n",
      " [2 0 2 0 0]\n",
      " [0 2 0 2 0]]\n",
      "idx_hidden_layer : 1, num_hidden_nodes : 2\n",
      "idx_hidden_row : [3, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WANNFCN(mat_wann5, activations)\n",
    "numpy_input = np.array([[1,2,3,4,5],\n",
    "                        [6,7,8,9,10],\n",
    "                        [11,12,13,14,15]])\n",
    "numpy_input = torch.from_numpy(numpy_input).float()\n",
    "model(numpy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
