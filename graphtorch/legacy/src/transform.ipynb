{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import torch  \n",
    "from torch import nn    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guideline\n",
    "\n",
    "![](example_guideline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "[https://discuss.pytorch.org/t/append-for-nn-sequential-or-directly-converting-nn-modulelist-to-nn-sequential/7104/4](https://discuss.pytorch.org/t/append-for-nn-sequential-or-directly-converting-nn-modulelist-to-nn-sequential/7104/4)    \n",
    "\n",
    "[https://discuss.pytorch.org/t/custom-connections-in-neural-network-layers/3027/15](https://discuss.pytorch.org/t/custom-connections-in-neural-network-layers/3027/15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input layer랑 output layer만 있는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래 코드를 좀 변형하면 될듯? 아래 코드는 hidden layer가 없어서.. \n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, indices_mask):\n",
    "        \"\"\"\n",
    "       :param in_features: number of input features\n",
    "       :param out_features: number of output features\n",
    "       :param indices_mask: list of two lists containing indices for dimensions 0 and 1, used to create the mask  \n",
    "       \"\"\"\n",
    "        super(MaskedLinear, self).__init__()    \n",
    "        \n",
    " \n",
    "        def backward_hook(grad): \n",
    "            # Clone due to not being allowed to modify in-place gradients\n",
    "            out = grad.clone()  \n",
    "            out[self.mask] = 0    \n",
    "            return out \n",
    " \n",
    "        self.linear = nn.Linear(in_dim, out_dim)#.cuda()    \n",
    "        self.mask = torch.ones([out_dim, in_dim]).byte()#.cuda()\n",
    "        self.mask[indices_mask] = 0 # create mask\n",
    "        self.linear.weight.data[self.mask] = 0 # zero out bad weights\n",
    "        self.linear.weight.register_hook(backward_hook) # hook to zero out bad gradients\n",
    " \n",
    "    def forward(self, input):\n",
    "        return self.linear(input)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0],\n",
       "        [1, 0, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_mask = torch.ones([2,4]).byte() #[output_dim, input_dim] \n",
    "indices_mask[0,0] = 0  # [output_index, input_index]: gradient 0으로 해주고 싶은 부분 \n",
    "indices_mask[1,1] = 0 \n",
    "indices_mask[0,2] = 0\n",
    "indices_mask[0,3] = 0\n",
    "indices_mask[1,3] = 0\n",
    "indices_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedLinear(4, 2, indices_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0397,  0.0000,  0.0000],\n",
       "        [-0.0415,  0.0000,  0.3291,  0.0000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hidden layer까지 있는 경우  \n",
    "- adjacency matrix의 transpose 버전인 mat_mask를 통해서 hidden layer들의 dimension을 구한다 \n",
    "- mat_mask의 row가 hidden과 output, column이 input과 hidden \n",
    "- 나머지 부분 바꿔서 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 0 2 0 0 0 0 0]\n",
      " [2 0 2 0 0 0 0 0 0 0]\n",
      " [0 2 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3]\n",
      " [0 0 0 0 0 0 0 0 3 0]]\n",
      "(7, 10)\n"
     ]
    }
   ],
   "source": [
    "# mat_mask 행렬 바로 생성 \n",
    "mat_mask = np.array([[0,2,0,0,2,0,0,0,0,0],[2,0,2,0,0,0,0,0,0,0],[0,2,0,2,0,0,0,0,0,0],[0,0,0,0,0,1,1,0,0,0],[0,0,0,0,0,0,1,1,0,0],[0,0,0,0,0,0,0,0,0,3],[0,0,0,0,0,0,0,0,3,0]])\n",
    "print(mat_mask)\n",
    "print(mat_mask.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 0, 0, 2, 0, 0, 0, 0, 0],\n",
      "        [2, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 2, 0, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 3, 0]], dtype=torch.int32)\n",
      "torch.Size([7, 10])\n"
     ]
    }
   ],
   "source": [
    "mat_mask = torch.from_numpy(mat_mask)\n",
    "print(mat_mask)\n",
    "print(mat_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 0, 0, 2],\n",
       "        [2, 0, 2, 0, 0],\n",
       "        [0, 2, 0, 2, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_mask[0:3, 0:5] #index (0~2, 0~4) 표현할라면 이렇게.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_mask[3,0:5].sum() == 0\n",
    "#이때 3이 첫번째 hidden layer의 dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer 추가한 버전\n",
    "#아래 코드를 좀 변형하면 될듯? 아래 코드는 hidden layer가 없어서.. \n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, mat_mask):\n",
    "        \"\"\"\n",
    "       :param in_features: number of input features\n",
    "       :param out_features: number of output features\n",
    "       :param indices_mask: list of two lists containing indices for dimensions 0 and 1, used to create the mask  \n",
    "       \"\"\"\n",
    "        super(MaskedLinear, self).__init__()    \n",
    "        \n",
    "        def calculate_hidden_dim(in_dim, out_dim, mat_mask):\n",
    "            #hidden dim의 정보를 담은 list를 출력한다..?\n",
    "            hidden_dim_list = []\n",
    "            start_col_idx = 0 \n",
    "            finish_col_idx = in_dim - 1\n",
    "            \n",
    "            while(True):\n",
    "                if finish_col_idx >= mat_mask.shape[1]:\n",
    "                    break \n",
    "        \n",
    "                for i in range(start_col_idx, len(mat_mask)): \n",
    "                    if (mat_mask[i,start_col_idx:(finish_col_idx + 1)].sum() == 0):\n",
    "                        hidden_dim = i - sum(hidden_dim_list)\n",
    "                        hidden_dim_list += [hidden_dim]  \n",
    "                        start_col_idx = finish_col_idx + 1\n",
    "                        finish_col_idx += i \n",
    "                        break \n",
    "                        \n",
    "            return hidden_dim_list\n",
    " \n",
    "        def backward_hook(grad): \n",
    "            # Clone due to not being allowed to modify in-place gradients     \n",
    "            out = grad.clone()    \n",
    "            out[self.mask] = 0    \n",
    "            return out  \n",
    "        \n",
    "        \n",
    "        def mask_grad(from_dim, to_dim, indices_mask): #두 layer에 대해서 gradient masking 해주고 생성한 layer를 return\n",
    "            \n",
    "            if (indices_mask == 1).sum() != 0: #indices_mask가 1을 가지고 있으면#왜냐하면 layer사이에서의 activation function은 같으니까\n",
    "                self.layer = nn.Linear(from_dim, to_dim)\n",
    "\n",
    "            elif (indices_mask == 2).sum() != 0: #indices_mask가 2를 가지고 있으면  \n",
    "                self.layer = nn.Sequential(nn.Linear(from_dim, to_dim), nn.ReLU(True))    \n",
    "            \n",
    "            elif (indices_mask == 3).sum() != 0: #indices_mask가 3을 가지고 있으면    \n",
    "                self.layer = nn.Sequential(nn.Linear(from_dim, to_dim), nn.Sigmoid(True)) \n",
    "            \n",
    "            self.mask = torch.ones([to_dim, from_dim]).byte()\n",
    "            self.mask[indices_mask] = 0\n",
    "            self.layer.weight.data[self.mask] = 0 \n",
    "            self.layer.weight.register_hook(backward_hook)   \n",
    "            \n",
    "            return self.layer  \n",
    "            \n",
    "        #이부분 바꿔야함     \n",
    "        hidden_dim_list = calculate_hidden_dim(in_dim, out_dim, mat_mask)    \n",
    "        \n",
    "        #input이랑 첫번째 hidden layer\n",
    "        self.input = mask_grad(in_dim, hidden_dim_list[0], mat_mask[0:hidden_dim_list[0], 0:in_dim])  \n",
    "        self.hiddens = nn.Sequential()  \n",
    "        for i in range(len(hidden_dim_list)-1): # repetition by (# of layers -1)  \n",
    "            self.layer = mask_grad(hidden_dim_list[i], hidden_dim_list[i+1], mat_mask[hidden_dim_list[0:i-1].sum()-1:hidden_dim_list[0:i].sum()-1,in_dim + hidden_dim_list[0:i-1].sum()-1:in_dim + hidden_dim_list[0:i].sum()-1])    \n",
    "            self.hiddens.add_module(\"hidden{}\".format(i+1), self.layer)   \n",
    "                \n",
    "        self.output = mask_grad(hidden_dim_list[len(hidden_dim_list)-1], out_dim, mat_mask[(mat_mask.shape[0]-out_dim):,(mat_mask.shape[1]-hidden_dim_list[len(hidden_dim_list)-1]):])  \n",
    "                \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    #이부분 바꿔야함    \n",
    "    def forward(self, input):   \n",
    "       \n",
    "        o1 = self.layer1(input)     \n",
    "        o2 = self.hiddens(o1)       \n",
    "        out = self.output(o2)      \n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d44db5a5a849>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaskedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-6e25ba0abf61>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_dim, out_dim, mat_mask)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m#input이랑 첫번째 hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhidden_dim_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0min_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhiddens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_dim_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# repetition by (# of layers -1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6e25ba0abf61>\u001b[0m in \u001b[0;36mmask_grad\u001b[1;34m(from_dim, to_dim, indices_mask)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackward_hook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    537\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 539\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "model = MaskedLinear(5, 2, mat_mask.byte())       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []  \n",
    "temp += [1]   \n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp += [2]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(mat_mask.shape[0])\n",
    "print(mat_mask.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_list = []\n",
    "start_col_idx = 0 \n",
    "finish_col_idx = 4 # 5 - 1\n",
    "            \n",
    "while(True):   \n",
    "    \n",
    "    if finish_col_idx >= mat_mask.shape[1]:   \n",
    "        break    \n",
    "        \n",
    "    for i in range(start_col_idx, len(mat_mask)):       \n",
    "        if (mat_mask[i,start_col_idx:(finish_col_idx + 1)].sum() == 0):      \n",
    "            hidden_dim = i - sum(hidden_dim_list)    \n",
    "            hidden_dim_list += [hidden_dim]   \n",
    "            start_col_idx = finish_col_idx + 1  \n",
    "            finish_col_idx += i   \n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "8\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(hidden_dim_list)\n",
    "print(start_col_idx)\n",
    "print(finish_col_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim_list = []\n",
    "sum(hidden_dim_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mat_mask[0:3, 0:5] == 2).sum() != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1\n"
     ]
    }
   ],
   "source": [
    "print(\"hidden{}\".format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(hidden_dim_list[0:2])\n",
    "print(sum(hidden_dim_list[0:2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
